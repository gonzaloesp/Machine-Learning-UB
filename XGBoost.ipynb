{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST\n",
    "\n",
    "This notebook provide an explanation from scratch and user guide for Extreme Gradient Boosting, one of the most implementations of Graadient Boosting. XGboost is an optimized gradient-boosting machine learning library, originally written in C++, that has APIs in several languages: Python, R, Scala, Julia and Java. XGBoost is that popular because of speed and performance. Unlike other Gradient Boosting implementations, XGBoost is parallelizable, what allows train large datasets and as ensemble method it consistently outperforms single-algorithm methods.\n",
    "\n",
    "In order to understand this implementation, we are going to revise Decision Trees and Boosting and therefore get into XGBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of decision trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. \n",
    "\n",
    "Starting from a node that contain all data (root), decision rules (binary questions) split the data -using axis orthogonal hyperplanes- into two subsets. One of these subsets is a leaf node (corresponds to a class label). The other becomes an internal node where the next decision rule is applied (i.e. at each internal node we test a value of a feature). This strategy is applied till all nodes are leafs or the algorithm is early-stopped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.researchgate.net/profile/Friedhelm_Schwenker/publication/220361040/figure/fig1/AS:305421668634641@1449829608685/Fig-1-A-binary-decision-tree-of-depth-4-with-two-features-denoted-by-x-and-y-is.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://www.researchgate.net/profile/Friedhelm_Schwenker/publication/220361040/figure/fig1/AS:305421668634641@1449829608685/Fig-1-A-binary-decision-tree-of-depth-4-with-two-features-denoted-by-x-and-y-is.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very intuitive, isn`t it? However, the following two questions may appear:\n",
    "\n",
    "- Which features we should consider as decision rules? Which threshold we should apply? At which level we apply each decision rule?\n",
    "\n",
    "    There are many different splitting criteria. The most common ones are:\n",
    "    - Misclassification error: select the split that corrects more data at each point.\n",
    "    - Gini impurity: probabilistically model the notion of impurity of a node. $\\textit{Gini}: \\mathit{Gini}(E) = 1 - \\sum_{j=1}^{c}p_j^2$\n",
    "    - Cross-entropy/Information gain/Mutual information: probabilistically model the notion of impurity of a node. $\\textit{Entropy}: H(E) = -\\sum_{j=1}^{c}p_j\\log p_j$\n",
    "    *Note: Gini impurity and Cross-entropy are more or less the same (same result). They are interchangeable.*  \n",
    "\n",
    "\n",
    "- If we let the tree grow till the end, the training data is perfectly classified, which means overfitting. How do we deal with this fact?\n",
    "\n",
    "    There are two ways of avoiding overfitting in trees:\n",
    "    - Stop growing the tree when the split is not statistically significant (stopping criterion).\n",
    "    - Grow a full tree and post-prune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost uses a special type of trees called CART (Classification and Regression Trees). In spite of leaf containing decision values, in CART each leaf always contains a real-valued score. The main advantage is that they allow to deal both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Eve Calls</th>\n",
       "      <th>Eve Charge</th>\n",
       "      <th>Night Mins</th>\n",
       "      <th>Night Calls</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91.0</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123.0</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103.0</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114.0</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71.0</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89.0</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113.0</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Account Length  Int'l Plan  VMail Plan  VMail Message  Day Mins  Day Calls  \\\n",
       "0           128.0         0.0         1.0           25.0     265.1      110.0   \n",
       "1           107.0         0.0         1.0           26.0     161.6      123.0   \n",
       "2           137.0         0.0         0.0            0.0     243.4      114.0   \n",
       "3            84.0         1.0         0.0            0.0     299.4       71.0   \n",
       "4            75.0         1.0         0.0            0.0     166.7      113.0   \n",
       "\n",
       "   Day Charge  Eve Mins  Eve Calls  Eve Charge  Night Mins  Night Calls  \\\n",
       "0       45.07     197.4       99.0       16.78       244.7         91.0   \n",
       "1       27.47     195.5      103.0       16.62       254.4        103.0   \n",
       "2       41.38     121.2      110.0       10.30       162.6        104.0   \n",
       "3       50.90      61.9       88.0        5.26       196.9         89.0   \n",
       "4       28.34     148.3      122.0       12.61       186.9        121.0   \n",
       "\n",
       "   Night Charge  Intl Mins  Intl Calls  Intl Charge  CustServ Calls  y  \n",
       "0         11.01       10.0         3.0         2.70             1.0  0  \n",
       "1         11.45       13.7         3.0         3.70             1.0  0  \n",
       "2          7.32       12.2         5.0         3.29             0.0  0  \n",
       "3          8.86        6.6         7.0         1.78             2.0  0  \n",
       "4          8.41       10.1         3.0         2.73             3.0  0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn = pd.read_csv(\"churn.csv\")\n",
    "\n",
    "## preprocessing \n",
    "# We don't need these columns\n",
    "to_drop = ['State','Area Code','Phone','Churn?']\n",
    "churn_data = churn.drop(to_drop,axis=1)\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_data[yes_no_cols] = churn_data[yes_no_cols] == 'yes'\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_data.columns\n",
    "\n",
    "X = churn_data.as_matrix().astype(np.float)\n",
    "y = np.where(churn['Churn?']==\"True.\", 1,0)\n",
    "\n",
    "Xy = pd.DataFrame(X)\n",
    "Xy.columns = churn_data.columns\n",
    "Xy[\"y\"] = y\n",
    "Xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9016786570743405\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: \n",
    "dt_clf = DecisionTreeClassifier(max_depth = 3)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf.fit(X_train,  y_train)\n",
    "\n",
    "# Predict the labels of the test set:\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data = pd.read_csv(\"BostonHousing.csv\")\n",
    "housing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 40.4345069189\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = housing_data.drop(\"medv\",axis=1)\n",
    "y = housing_data[\"medv\"]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_reg = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_reg.fit(X_train,  y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "dt_reg = dt_reg.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "rmse = mean_squared_error(dt_reg, y_test)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a meta-algorithm (a concept that can be applied to a set of machine learning models) used to convert many weak learners into a strong learner.\n",
    "\n",
    "Boosting is a stage-wise additive modelling. Starts without a classifier and a first weak classifier is fit to the data. Another weak classifier is fitted to improve the current model performance, without changing the past classifiers and so on. This new classifier has to take into account where the pasts classifiers are not performing well. Given the previous classifiers, the next classifier is the one that decreases the most the objective function.\n",
    "\n",
    "In general terms boosting could be understood as: given a loss function, a response variable and a current model, at each state $t$ figure out the best improvement to the current model with another classifier and update the model with this classifier. This update can be regularized before it is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "The idea is iteratively learning a set of weak models on subsets of the data, weighing each weak prediction according to each weak learner's performance, and combining the weighted predictions to obtain a single weighted prediction that is much better than the individual predictions themselves.\n",
    "\n",
    "Base Learners in XGBoost\n",
    "- Linear Base Learner (rarely used):\n",
    "    - Sum of linear terms\n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear)\n",
    "- Tree Base Learner:\n",
    "    - Decision tree\n",
    "    - Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - Almost exclusively used in XGBoost\n",
    "\n",
    "Loss function names in xgboost:\n",
    "- reg:linear - use for regression problems\n",
    "- reg:logistic - use for classification problems when you want just decision, not probability\n",
    "- binary:logistic - use when you want probability rather than just decision\n",
    "\n",
    "Regularization parameters in XGBoost:\n",
    "- gamma - minimum loss reduction allowed for a split to occur\n",
    "- alpha - l1 regularization on leaf weights, larger values mean more regularization\n",
    "- lambda - l2 regularization on leaf weights\n",
    "\n",
    "When to use XGBoost:  \n",
    "- Practicable to any supervised task. You have a mixture of categorical and numeric features or just numeric features\n",
    "- You have a large number of training samples. When the number of training samples is large with respect to number of features \n",
    "\n",
    "When to NOT use XGBoost:  \n",
    "- Image recognition\n",
    "- Computer vision\n",
    "- Natural language processing and understanding problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.907674\n"
     ]
    }
   ],
   "source": [
    "# Create data\n",
    "X = churn_data.as_matrix().astype(np.float)\n",
    "y = np.where(churn['Churn?']==\"True.\", 1,0)\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123, max_depth = 3)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.565767\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = housing_data.drop(\"medv\",axis=1)\n",
    "y = housing_data[\"medv\"]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123, max_depth = 3)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_reg.fit(X_train,  y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the black box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importance**\n",
    "\n",
    "A benefit of using gradient boosting is that after the boosted trees are constructed, it is relatively straightforward to retrieve importance scores for each attribute.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.\n",
    "\n",
    "This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n",
    "\n",
    "Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function.\n",
    "\n",
    "The feature importances are then averaged across all of the the decision trees within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22305255cf8>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEWCAYAAABFSLFOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUTAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223050df358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing XGBoost Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "- tree tunable parameters:\n",
    "    - learning rate: learning rate/eta\n",
    "    - gamma: min loss reduction to create new tree split\n",
    "    - lambda: L2 reg on leaf weights\n",
    "    - alpha: L1 reg on leaf weights\n",
    "    - max_depth: max depth per tree\n",
    "    - subsample: % samples used per tree\n",
    "    - colsample_bytree: % features used per tree\n",
    "    - number of boosting rounds\n",
    "- linear classifier:\n",
    "    - lambda: L2 reg on weights\n",
    "    - alpha: L1 reg on weights\n",
    "    - lambda_bias: L2 reg term on bias\n",
    "    - number of boosting rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search\n",
    "Search exhaustively over a given set of hyperparameters, once per set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  4.67750914409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search\n",
    "Create a range of hyperparameter values per hyperparameter that you would like to search over and set the number of combinations you would like for the random search to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 3, 'learning_rate': 0.35000000000000003}\n",
      "Lowest RMSE found:  4.45482879543\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'learning_rate': np.arange(0.05,1.05,.05),\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, scoring = \"neg_mean_squared_error\",\n",
    "                                    n_iter = 10, cv = 4)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
